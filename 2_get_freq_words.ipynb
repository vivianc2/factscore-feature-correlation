{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_set = {'organist', 'narrator', 'pastor', 'musician', 'narration', 'author', 'arranger', 'educator', 'military officer', 'soloist', 'model', 'naval surgeon', 'fitness instructor', 'broadcasting', 'composer', 'data scientist', 'economist', 'journalist', 'politician', 'host', 'film director', 'guitarist', 'environmentalist', 'songwriter', 'lawyer', 'radio broadcaster', 'screenwriter', 'athlete', 'coach', 'revolutionary', 'essayist', 'comedian', 'locksmith', 'writer', 'record producer', 'entertainer', 'dancer', 'stage', 'media executive', 'actress', 'parliamentarian', 'poet', 'businessman', 'model', 'actor', 'tv personality', 'songwriter', 'professor', 'mountaineer', 'radio host', 'travel writer', 'sportsperson', 'producer', 'film actress', 'philanthropist', 'businesswoman', 'voice actor', 'geographer', 'director', 'architect', 'teacher', 'television host', 'playwright', 'animal-rights activist', 'singer', 'translator', 'novelist', 'rapper', 'deejay', 'film producer', 'entrepreneur', 'stuntman', 'sportsman', 'columnist'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(occupations_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_infobox_line(line):\n",
    "    \"\"\"Extracts and concatenates occupations from the infobox line.\"\"\"\n",
    "    occupations = set()\n",
    "    tokens = line.split('\\t')\n",
    "    occupation_parts = []\n",
    "    for token in tokens:\n",
    "        if 'occupation_' in token:\n",
    "            # Extract the part of the occupation from the token\n",
    "            part = token.split(':', 1)[1].strip()\n",
    "            part = part.replace('[[', '').replace(']]', '').replace('*', '').strip()\n",
    "            occupation_parts.append(part)\n",
    "\n",
    "    # Join all parts into one string and add to the set of occupations\n",
    "    if occupation_parts:\n",
    "        full_occupation = \" \".join(occupation_parts)\n",
    "        occupations.add(full_occupation)\n",
    "\n",
    "    return occupations\n",
    "\n",
    "\n",
    "def remove_substrings(occupation_list):\n",
    "    result = []\n",
    "    for occupation in occupation_list:\n",
    "        if not any(occupation in other for other in occupation_list if occupation != other):\n",
    "            result.append(occupation)\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_articles_by_occupation(dataset_dir, occupations_set, char_threshold, num_samples):\n",
    "    \"\"\"Collects articles based on specified occupations up to a sample limit.\"\"\"\n",
    "    articles = defaultdict(list)\n",
    "    subsets = ['train/train', 'valid/valid', 'test/test']\n",
    "    title_files = [os.path.join(dataset_dir, f\"{subset}.title\") for subset in subsets]\n",
    "    box_files = [os.path.join(dataset_dir, f\"{subset}.box\") for subset in subsets]\n",
    "    sent_files = [os.path.join(dataset_dir, f\"{subset}.sent\") for subset in subsets]\n",
    "    nb_files = [os.path.join(dataset_dir, f\"{subset}.nb\") for subset in subsets]\n",
    "\n",
    "    for idx, (title_file, box_file, sent_file, nb_file) in enumerate(zip(title_files, box_files, sent_files, nb_files)):\n",
    "        with open(title_file, 'r') as tfile, open(box_file, 'r') as bfile, open(sent_file, 'r') as sfile, open(nb_file, 'r') as nfile:\n",
    "            title_lines = tfile.readlines()\n",
    "            sent_lines = sfile.readlines()\n",
    "            nb_lines = [int(line.strip()) for line in nfile.readlines()]\n",
    "            start_index = 0\n",
    "            for i, (bline, num_sentences) in enumerate(zip(bfile, nb_lines)):\n",
    "                if all(len(articles[occ]) >= num_samples for occ in occupations_set):\n",
    "                    break\n",
    "                # print(\"bline:\", bline)\n",
    "                entry_occupations = parse_infobox_line(bline)\n",
    "                # print(\"entry:\", entry_occupations)\n",
    "                matched_occupations = [o for o in occupations_set if any(o in occ.lower() for occ in entry_occupations)]\n",
    "                # print(\"matched\", matched_occupations)\n",
    "                matched_occupations = remove_substrings(matched_occupations)\n",
    "                # print(\"matched2\", matched_occupations)\n",
    "                if matched_occupations:\n",
    "                    summary = ' '.join(sent_lines[start_index:start_index + num_sentences]).strip()\n",
    "                    name = title_lines[i].strip()\n",
    "                    if len(summary) > char_threshold:\n",
    "                        for occ in matched_occupations:\n",
    "                            if len(articles[occ]) < num_samples:\n",
    "                                articles[occ].append({'name': name, 'summary': summary})\n",
    "                                if len(articles[occ]) == num_samples:\n",
    "                                    break\n",
    "                start_index += num_sentences\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_and_report(articles, data_output_path, report_output_path, attribute_output_path):\n",
    "    \"\"\"Saves detailed data to JSON and generates a report on frequent words, also saved to JSON.\"\"\"\n",
    "    # Set up stopwords and lemmatizer\n",
    "    standard_stop_words = set(stopwords.words('english'))\n",
    "    custom_stop_words = [\"born\", \"known\", \"also\", \"became\", \"name\", \"one\"]\n",
    "    stop_words = standard_stop_words.union(set(custom_stop_words))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    data = {}\n",
    "    report_data = {}\n",
    "    attribute_data = {}\n",
    "\n",
    "    for occupation, entries in articles.items():\n",
    "        word_counter = Counter()\n",
    "        for entry in entries:\n",
    "            words = [\n",
    "                lemmatizer.lemmatize(word.lower())  # Lemmatize the word\n",
    "                for word in word_tokenize(entry['summary'])\n",
    "                if word.lower() not in stop_words and word.isalpha()\n",
    "            ]\n",
    "            word_counter.update(words)\n",
    "        \n",
    "        most_common_words = word_counter.most_common(20)\n",
    "        data[occupation] = {\n",
    "            'count': len(entries),\n",
    "            'entries': entries\n",
    "        }\n",
    "        report_data[occupation] = {\n",
    "            'count': len(entries),\n",
    "            'frequent_words': [{word: count} for word, count in most_common_words]\n",
    "        }\n",
    "        attribute_data[occupation] = {\n",
    "            'attribute': [word for word, _ in most_common_words]\n",
    "        }\n",
    "    \n",
    "    # Print and save data\n",
    "    print(len(report_data))\n",
    "    print(report_data.keys())\n",
    "\n",
    "    with open(data_output_path, 'w', encoding='utf-8') as f_data:\n",
    "        json.dump(data, f_data, indent=4)\n",
    "\n",
    "    with open(report_output_path, 'w', encoding='utf-8') as f_report:\n",
    "        json.dump(report_data, f_report, indent=4)\n",
    "\n",
    "    with open(attribute_output_path, 'w', encoding='utf-8') as f_attribute:\n",
    "        json.dump(attribute_data, f_attribute, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "dict_keys(['author', 'actress', 'businesswoman', 'politician', 'composer', 'musician', 'arranger', 'film director', 'actor', 'essayist', 'writer', 'poet', 'playwright', 'songwriter', 'singer', 'travel writer', 'director', 'teacher', 'lawyer', 'screenwriter', 'record producer', 'film producer', 'architect', 'model', 'comedian', 'novelist', 'producer', 'professor', 'journalist', 'translator', 'coach', 'host', 'organist', 'rapper', 'radio host', 'entrepreneur', 'athlete', 'philanthropist', 'businessman', 'economist', 'revolutionary', 'environmentalist', 'stage', 'tv personality', 'dancer', 'educator', 'guitarist', 'entertainer', 'voice actor', 'columnist', 'narrator', 'television host', 'media executive', 'sportsman', 'broadcasting', 'film actress', 'military officer', 'pastor', 'stuntman', 'radio broadcaster', 'deejay', 'geographer', 'mountaineer', 'soloist', 'sportsperson', 'fitness instructor', 'data scientist', 'parliamentarian', 'locksmith', 'narration', 'naval surgeon'])\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = 'wikipedia-biography-dataset/wikipedia-biography-dataset'\n",
    "char_threshold = 200\n",
    "num_samples = 100\n",
    "data_output_path = 'output/occupation_data_100.json'\n",
    "report_output_path = 'output/occupation_report_100.json'\n",
    "attribute_output_path = 'output/occupation_attribute_100.json'\n",
    "\n",
    "articles_by_occupation = filter_articles_by_occupation(dataset_dir, occupations_set, char_threshold, num_samples)\n",
    "save_data_and_report(articles_by_occupation, data_output_path, report_output_path, attribute_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factscore",
   "language": "python",
   "name": "factscore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
