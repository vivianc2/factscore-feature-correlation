{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_set(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    feature_set = set(data.keys())\n",
    "    return feature_set\n",
    "\n",
    "file_path = 'output/freq_words/frequent_word_1000.json'\n",
    "feature_set = get_feature_set(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_jsonl(jsonl_files):\n",
    "    data = []\n",
    "    for file_path in jsonl_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Each line is a complete JSON object\n",
    "                entry = json.loads(line)\n",
    "                data.append(entry)\n",
    "    return data\n",
    "\n",
    "# Assuming you have paths to your JSONL files\n",
    "labled_path = 'data/labeled/'\n",
    "jsonl_files = [labled_path+'ChatGPT.jsonl', labled_path+'InstructGPT.jsonl', labled_path+'PerplexityAI.jsonl']\n",
    "all_data = load_data_from_jsonl(jsonl_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeld_output_list = [i['output'] for i in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lanny Flaherty is an American actor born on December 18, 1949, in Pensacola, Florida. He has appeared in numerous films, television shows, and theater productions throughout his career, which began in the late 1970s. Some of his notable film credits include \"King of New York,\" \"The Abyss,\" \"Natural Born Killers,\" \"The Game,\" and \"The Straight Story.\" On television, he has appeared in shows such as \"Law & Order,\" \"The Sopranos,\" \"Boardwalk Empire,\" and \"The Leftovers.\" Flaherty has also worked extensively in theater, including productions at the Public Theater and the New York Shakespeare Festival. He is known for his distinctive looks and deep gravelly voice, which have made him a memorable character actor in the industry.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeld_output_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_jsonl(jsonl_files):\n",
    "    data = []\n",
    "    for file_path in jsonl_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Each line is a complete JSON object\n",
    "                entry = json.loads(line)\n",
    "                data.append(entry)\n",
    "    return data\n",
    "\n",
    "# Assuming you have paths to your JSONL files\n",
    "labled_path = 'data/unlabeled/'\n",
    "file_list = ['Alpaca-7B.jsonl', 'MPT-Chat-7B.jsonl', 'Vicuna-7B.jsonl']\n",
    "jsonl_files = [labled_path+i for i in file_list]\n",
    "all_data_s = load_data_from_jsonl(jsonl_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeld_output_list += [i['output'] for i in all_data_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_jsonl(jsonl_files):\n",
    "    data = []\n",
    "    for file_path in jsonl_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Each line is a complete JSON object\n",
    "                entry = json.loads(line)\n",
    "                data.append(entry)\n",
    "    return data\n",
    "\n",
    "# Assuming you have paths to your JSONL files\n",
    "labled_path = 'factscore-unlabeled-predictions/'\n",
    "file_list = ['alpaca-7B.jsonl', 'mpt-7b.jsonl', 'vicuna-7b.jsonl']\n",
    "jsonl_files = [labled_path+i for i in file_list]\n",
    "all_data_m = load_data_from_jsonl(jsonl_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'facts': ['Jake Tilson is 22 years old.',\n",
       "  'Jake Tilson is an entrepreneur.',\n",
       "  'Jake Tilson is from the United States.',\n",
       "  'He is the founder.',\n",
       "  'He is the CEO.',\n",
       "  'He is the founder and CEO of several successful startups.',\n",
       "  'He is the founder and CEO of a social media marketing agency.',\n",
       "  'He is the founder and CEO of a mobile app development company.',\n",
       "  'He is passionate about technology.',\n",
       "  'He loves to stay up to date.',\n",
       "  'He loves to stay up to date with the latest trends.',\n",
       "  'He loves to stay up to date with the latest trends in the industry.',\n",
       "  'He is an avid traveler.',\n",
       "  'He loves to explore new places.'],\n",
       " 'ChatGPT_Labels': ['NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS'],\n",
       " 'LLAMA+NP_Labels': ['NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS',\n",
       "  'NS'],\n",
       " 'prompt': 'Question: Tell me a bio of Jake Tilson.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2049"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeld_output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jackie Shroff is an Indian film actor who has appeared in Hindi, Tamil, Telugu, Malayalam, and Kannada language films. He is known for his versatility and has played a wide range of roles in his career.\\n\\nShroff was born on March 1, 1957, in Bombay, India. His father was a cinematographer and his mother was a stage actress. Shroff began his career in the film industry as a child artist and made his adult debut in the film \"Swami Dada\" in 1978.\\n\\nOver the years, Shroff has played a variety of roles in films, ranging from action heroes to romantic leads to comedic characters. He has worked with many leading directors in the Hindi film industry and has been part of several successful films. Some of his notable films include \"Tiger Zinda Hai,\" \"Baaghi 2,\" \"Khalnayak,\" \"Mohra,\" and \"Darr.\"\\n\\nShroff is known for his good looks and charisma and has been a popular actor in the Indian film industry for many years. He has received several awards for his work, including the Filmfare Award for Best Supporting Actor for his performance in the film \"Parinda.\"\\n\\nShroff is also involved in social and humanitarian causes and has supported various charities and organizations over the years. He is a popular figure in the entertainment industry and continues to act in films to this day.',\n",
       " 'Cha Eun-woo is a South Korean singer and actor. He was born on October 12, 1996, in Busan, South Korea. He is a member of the K-pop group EXO, which was formed by SM Entertainment in 2011.\\n\\nCha made his debut as a member of EXO in 2012 with the release of their debut album \"XOXO.\" Since then, he has participated in the group\\'s various activities, including album releases, concert tours, and television appearances.\\n\\nIn addition to his work with EXO, Cha has also pursued a career as an actor. He made his acting debut in the 2016 television series \"Splendid Politics,\" and has since appeared in various other dramas and films.\\n\\nCha is known for his good looks and talents in singing and acting. He is also a philanthropist and has been involved in various charity events and causes.\\n\\nIn his personal life, Cha is known to be a private person and has not shared much information about his personal life with the public. He is reported to be in a relationship with actress Kim So-eun, with whom he starred in the 2020 television series \"Radiant Office.\"',\n",
       " \"Nick Kyrgios is an Australian professional tennis player known for his aggressive playing style and flamboyant personality. He was born on May 27, 1995, in Sydney, Australia, and began playing tennis at a young age.\\n\\nKyrgios quickly gained attention on the tennis circuit for his powerful serves and aggressive playing style. In 2013, he won the boys' singles title at the Australian Open, cementing his status as a rising star in the sport.\\n\\nKyrgios has achieved several notable victories throughout his career, including reaching the quarterfinals of the 2015 Wimbledon Championships and the 2014 US Open, where he defeated Rafael Nadal in a memorable match.\\n\\nDespite his success on the court, Kyrgios has also been known for his controversial behavior and often controversial remarks. In 2019, he was fined for unsportsmanlike conduct during a match at the Citi Open in Washington, D.C.\\n\\nOverall, Kyrgios is a talented and polarizing figure in the world of tennis, known for his passion and flair on the court, as well as his occasionally controversial behavior off it.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeld_output_list[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_probability_matrix_from_list, save_outputs, save_sorted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing occupations: 100%|██████████| 2049/2049 [00:06<00:00, 319.39it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_list_lab, prob_matrix_lab, feature_counts_lab, co_occurrence_counts_lab = build_probability_matrix_from_list(labeld_output_list, feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_outputs(feature_list_lab, prob_matrix_lab, feature_counts_lab, co_occurrence_counts_lab, 'output/wiki_vs_gen/new_features/lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prefix = 'output/wiki_vs_gen_report/new_features_1000/lab'\n",
    "output_file = output_prefix + '_sorted_probabilities_report.txt'\n",
    "save_sorted_probabilities(feature_list_lab, prob_matrix_lab, feature_counts_lab, co_occurrence_counts_lab, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_matrix_lab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Human Labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def analyze_hallucinations(data, feature_set, correlation_matrix):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    feature_list = sorted(list(feature_set))  # Ensure consistent ordering\n",
    "    index_map = {feature: idx for idx, feature in enumerate(feature_list)}\n",
    "\n",
    "    # Structures to store outputs\n",
    "    hallucination_leads = defaultdict(list)\n",
    "    hallucination_unique_leads = defaultdict(set)\n",
    "    hallucination_probabilities = defaultdict(dict)\n",
    "    appearance_matrix = np.zeros_like(correlation_matrix, dtype=bool)\n",
    "\n",
    "    for entry in data:\n",
    "        previous_features = []\n",
    "        annotations = entry.get(\"annotations\", [])\n",
    "        if annotations is not None:\n",
    "            for annotation in annotations:\n",
    "                text = annotation.get(\"text\", \"\").lower()\n",
    "                words = word_tokenize(text)\n",
    "                lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "                current_features = [word for word in lemmatized_words if word in feature_set]\n",
    "                previous_features.extend(current_features)\n",
    "\n",
    "                human_atomic_facts = annotation.get(\"human-atomic-facts\", [])\n",
    "                if human_atomic_facts is not None:\n",
    "                    is_hallucinated = any(fact['label'] == \"NS\" for fact in human_atomic_facts)\n",
    "                    if is_hallucinated:\n",
    "                        for feature in current_features:\n",
    "                            filtered_features = [f for f in previous_features] # if f != feature (I think we should include same ones?)\n",
    "                            hallucination_leads[feature].append(filtered_features)\n",
    "                            hallucination_unique_leads[feature].update(filtered_features)\n",
    "\n",
    "                            # Calculate and record conditional probabilities\n",
    "                            for f in filtered_features:\n",
    "                                f_idx = index_map[f]\n",
    "                                k_idx = index_map[feature]\n",
    "                                appearance_matrix[f_idx, k_idx] = True\n",
    "                                hallucination_probabilities[feature][f] = correlation_matrix[f_idx, k_idx]\n",
    "\n",
    "    return hallucination_leads, hallucination_unique_leads, hallucination_probabilities, appearance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_hallucinations(all_data, feature_set, prob_matrix_lab)\n",
    "hallucination_leads, hallucination_unique_leads, hallucination_probabilities, appearance_matrix = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(hall, fame): 0.09016393442622951\n",
      "(person, named): 0.07580645161290323\n",
      "(recipient, award): 0.07210031347962383\n",
      "(around, world): 0.06825297432686286\n",
      "(screen, award): 0.06687898089171974\n",
      "(screenplay, film): 0.06349206349206349\n",
      "(remembered, one): 0.06262626262626263\n",
      "(mtv, award): 0.06153846153846154\n",
      "(screen, actor): 0.05732484076433121\n",
      "(emmy, award): 0.05511811023622047\n",
      "(writes, character): 0.05263157894736842\n",
      "(writes, fantasy): 0.05263157894736842\n",
      "(writes, work): 0.05263157894736842\n",
      "(writes, working): 0.05263157894736842\n",
      "(writes, subsequently): 0.05263157894736842\n",
      "(writes, romance): 0.05263157894736842\n",
      "(writes, genre): 0.05263157894736842\n",
      "(writes, main): 0.05263157894736842\n",
      "(writes, fiction): 0.05263157894736842\n",
      "(writes, job): 0.05263157894736842\n",
      "(writes, historical): 0.05263157894736842\n",
      "(writes, dark): 0.05263157894736842\n",
      "(reality, show): 0.05233494363929147\n",
      "(critically, acclaimed): 0.050387596899224806\n",
      "(appears, film): 0.05\n",
      "(appears, several): 0.05\n",
      "(disney, film): 0.04918032786885246\n",
      "(directed, film): 0.04881656804733728\n",
      "(collaborated, artist): 0.048426150121065374\n",
      "(recurring, role): 0.04807692307692308\n"
     ]
    }
   ],
   "source": [
    "hallucination_pairs = [\n",
    "    (feature, hallucinated_feature, prob)\n",
    "    for hallucinated_feature, hallucinated_features in hallucination_probabilities.items()\n",
    "    for feature, prob in hallucinated_features.items()\n",
    "]\n",
    "\n",
    "sorted_pairs = sorted(hallucination_pairs, key=lambda item: item[2], reverse=True)\n",
    "\n",
    "top_n = 30\n",
    "for (feature, hallucinated_feature, prob) in sorted_pairs[:top_n]:\n",
    "    print(f\"({feature}, {hallucinated_feature}): {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "folder_name = 'output/wiki_vs_gen_report/new_features_1000/'\n",
    "df = pd.read_pickle(folder_name + 'comparison_df2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Wiki_Prob</th>\n",
       "      <th>Gen_Prob</th>\n",
       "      <th>Wiki_Bucket</th>\n",
       "      <th>Change</th>\n",
       "      <th>Change_Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>american</td>\n",
       "      <td>american</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.005791</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actor</td>\n",
       "      <td>american</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.000259</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>born</td>\n",
       "      <td>american</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.009162</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>december</td>\n",
       "      <td>american</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.013106</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.009601</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>florida</td>\n",
       "      <td>american</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.006940</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127446</th>\n",
       "      <td>world</td>\n",
       "      <td>date</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127447</th>\n",
       "      <td>dance</td>\n",
       "      <td>date</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127448</th>\n",
       "      <td>released</td>\n",
       "      <td>date</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127449</th>\n",
       "      <td>album</td>\n",
       "      <td>date</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.001131</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127450</th>\n",
       "      <td>date</td>\n",
       "      <td>date</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.012570</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127451 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature1  Feature2  Probability  Wiki_Prob  Gen_Prob  Wiki_Bucket  \\\n",
       "0       american  american     0.006025   0.004940  0.005791          5.0   \n",
       "1          actor  american     0.003055   0.002733  0.002474          5.0   \n",
       "2           born  american     0.003075   0.012286  0.003124          5.0   \n",
       "3       december  american     0.003675   0.013106  0.003505          5.0   \n",
       "4        florida  american     0.007160   0.007376  0.006940          5.0   \n",
       "...          ...       ...          ...        ...       ...          ...   \n",
       "127446     world      date     0.000133   0.000511  0.000040          3.0   \n",
       "127447     dance      date     0.001247   0.000664  0.000562          3.0   \n",
       "127448  released      date     0.000365   0.001383  0.000365          5.0   \n",
       "127449     album      date     0.000407   0.001493  0.000362          5.0   \n",
       "127450      date      date     0.010811   0.002449  0.012570          5.0   \n",
       "\n",
       "          Change Change_Direction  \n",
       "0       0.000851         Increase  \n",
       "1      -0.000259         Decrease  \n",
       "2      -0.009162         Decrease  \n",
       "3      -0.009601         Decrease  \n",
       "4      -0.000437         Decrease  \n",
       "...          ...              ...  \n",
       "127446 -0.000471         Decrease  \n",
       "127447 -0.000103         Decrease  \n",
       "127448 -0.001018         Decrease  \n",
       "127449 -0.001131         Decrease  \n",
       "127450  0.010120         Increase  \n",
       "\n",
       "[127451 rows x 8 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_df = pd.DataFrame(hallucination_pairs, columns=['Feature1', 'Feature2', 'Probability'])\n",
    "\n",
    "merged_df = pd.merge(hallucination_df, df, on=['Feature1', 'Feature2'], how='left')\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wiki_Bucket</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>19322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>28869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>59626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wiki_Bucket  Count\n",
       "0          1.0   6439\n",
       "1          2.0  13182\n",
       "2          3.0  19322\n",
       "3          4.0  28869\n",
       "4          5.0  59626"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_stats = merged_df.groupby('Wiki_Bucket')['Feature1'].count().reset_index()\n",
    "bucket_stats.columns = ['Wiki_Bucket', 'Count']\n",
    "bucket_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For model labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def analyze_hallucinations_m(data, feature_set, correlation_matrix):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    feature_list = sorted(list(feature_set))  # Ensure consistent ordering\n",
    "    index_map = {feature: idx for idx, feature in enumerate(feature_list)}\n",
    "\n",
    "    # Structures to store outputs\n",
    "    hallucination_leads = defaultdict(list)\n",
    "    hallucination_unique_leads = defaultdict(set)\n",
    "    hallucination_probabilities = defaultdict(dict)\n",
    "    appearance_matrix = np.zeros_like(correlation_matrix, dtype=bool)\n",
    "\n",
    "    for entry in data:\n",
    "        previous_features = []\n",
    "        annotations = entry.get(\"facts\", [])  # Changed to 'facts' and directly use it\n",
    "        chatgpt_labels = entry.get(\"ChatGPT_Labels\", [])\n",
    "        llama_labels = entry.get(\"LLAMA+NP_Labels\", [])\n",
    "        \n",
    "        for text, cg_label, llama_label in zip(annotations, chatgpt_labels, llama_labels):\n",
    "            words = word_tokenize(text.lower())\n",
    "            lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "            current_features = [word for word in lemmatized_words if word in feature_set]\n",
    "            previous_features.extend(current_features)\n",
    "\n",
    "            is_hallucinated = \"NS\" in (cg_label, llama_label)\n",
    "            if is_hallucinated:\n",
    "                for feature in current_features:\n",
    "                    filtered_features = [f for f in previous_features]  # including current feature as well\n",
    "                    hallucination_leads[feature].append(filtered_features)\n",
    "                    hallucination_unique_leads[feature].update(filtered_features)\n",
    "\n",
    "                    # Calculate and record conditional probabilities\n",
    "                    for f in filtered_features:\n",
    "                        f_idx = index_map[f]\n",
    "                        k_idx = index_map[feature]\n",
    "                        appearance_matrix[f_idx, k_idx] = True\n",
    "                        hallucination_probabilities[feature][f] = correlation_matrix[f_idx, k_idx]\n",
    "\n",
    "    return hallucination_leads, hallucination_unique_leads, hallucination_probabilities, appearance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m = analyze_hallucinations_m(all_data_m, feature_set, prob_matrix_lab)\n",
    "hallucination_leads_m, hallucination_unique_leads_m, hallucination_probabilities_m, appearance_matrix_m = results_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import numpy as np\n",
    "\n",
    "# def merge_results(orig_leads, model_leads, orig_uniques, model_uniques, orig_probs, model_probs, orig_matrix, model_matrix):\n",
    "#     # Merge hallucination leads\n",
    "#     combined_leads = defaultdict(list, orig_leads)\n",
    "#     for key, values in model_leads.items():\n",
    "#         combined_leads[key].extend(values)\n",
    "\n",
    "#     # Merge unique leads\n",
    "#     combined_uniques = defaultdict(set, orig_uniques)\n",
    "#     for key, values in model_uniques.items():\n",
    "#         combined_uniques[key].update(values)\n",
    "\n",
    "#     # Merge probabilities\n",
    "#     combined_probabilities = defaultdict(dict, orig_probs)\n",
    "#     for feature, subdict in model_probs.items():\n",
    "#         if feature in combined_probabilities:\n",
    "#             combined_probabilities[feature].update(subdict)\n",
    "#         else:\n",
    "#             combined_probabilities[feature] = subdict\n",
    "\n",
    "#     # Merge appearance matrices\n",
    "#     combined_matrix = orig_matrix + model_matrix\n",
    "\n",
    "#     return combined_leads, combined_uniques, combined_probabilities, combined_matrix\n",
    "\n",
    "\n",
    "# combined_leads, combined_uniques, combined_probabilities, combined_matrix = merge_results(\n",
    "#     hallucination_leads, hallucination_leads_m,\n",
    "#     hallucination_unique_leads, hallucination_unique_leads_m,\n",
    "#     hallucination_probabilities, hallucination_probabilities_m,\n",
    "#     appearance_matrix, appearance_matrix_m\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后: 前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_outputs(hallucination_details, hallucination_uniques, hallucination_probs, file_prefix):\n",
    "#     with open(f'{file_prefix}_details.json', 'w') as f:\n",
    "#         json.dump(hallucination_details, f, indent=4)\n",
    "#     with open(f'{file_prefix}_uniques.json', 'w') as f:\n",
    "#         # Convert sets in hallucination_uniques to lists for JSON serialization\n",
    "#         json.dump({k: list(v) for k, v in hallucination_uniques.items()}, f, indent=4)\n",
    "#     with open(f'{file_prefix}_probabilities.json', 'w') as f:\n",
    "#         # Assuming hallucination_probs uses dictionaries and does not need conversion\n",
    "#         json.dump(hallucination_probs, f, indent=4)\n",
    "\n",
    "# save_outputs(combined_leads, combined_uniques, combined_probabilities, 'output/hallu/combined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(hall, fame): 0.09016393442622951\n",
      "(person, named): 0.07580645161290323\n",
      "(recipient, award): 0.07210031347962383\n",
      "(around, world): 0.06825297432686286\n",
      "(screen, award): 0.06687898089171974\n",
      "(screenplay, film): 0.06349206349206349\n",
      "(remembered, one): 0.06262626262626263\n",
      "(mtv, award): 0.06153846153846154\n",
      "(screen, actor): 0.05732484076433121\n",
      "(emmy, award): 0.05511811023622047\n",
      "(reality, show): 0.05233494363929147\n",
      "(critically, acclaimed): 0.050387596899224806\n",
      "(directed, film): 0.04881656804733728\n",
      "(collaborated, artist): 0.048426150121065374\n",
      "(recurring, role): 0.04807692307692308\n",
      "(nominated, award): 0.04787812840043525\n",
      "(considered, one): 0.04750705550329257\n",
      "(motion, picture): 0.04716981132075472\n",
      "(grammy, award): 0.04693140794223827\n",
      "(legislative, assembly): 0.04693140794223827\n",
      "(gold, medal): 0.04552590266875981\n",
      "(regarded, one): 0.045512010113780026\n",
      "(acted, film): 0.045171339563862926\n",
      "(announced, would): 0.044444444444444446\n",
      "(died, age): 0.043560606060606064\n",
      "(present, death): 0.043478260869565216\n",
      "(married, child): 0.042473919523099854\n",
      "(produced, film): 0.042175360710321866\n",
      "(globe, award): 0.04205607476635514\n",
      "(graduate, university): 0.0420353982300885\n"
     ]
    }
   ],
   "source": [
    "hallucination_pairs_m = [\n",
    "    (feature, hallucinated_feature, prob)\n",
    "    for hallucinated_feature, hallucinated_features in hallucination_probabilities_m.items()\n",
    "    for feature, prob in hallucinated_features.items()\n",
    "]\n",
    "\n",
    "sorted_pairs = sorted(hallucination_pairs_m, key=lambda item: item[2], reverse=True)\n",
    "\n",
    "top_n = 30\n",
    "for (feature, hallucinated_feature, prob) in sorted_pairs[:top_n]:\n",
    "    print(f\"({feature}, {hallucinated_feature}): {prob}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Her death was widely believed to be related to her reporting on drug trafficking and corruption in Veracruz, and it sparked outrage and protests from journalists and human rights activists in Mexico and around the world.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. See what bucket hallucinated pairs are in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "folder_name = 'output/wiki_vs_gen_report/new_features_1000/'\n",
    "df = pd.read_pickle(folder_name + 'comparison_df2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Wiki_Prob</th>\n",
       "      <th>Gen_Prob</th>\n",
       "      <th>Wiki_Bucket</th>\n",
       "      <th>Change</th>\n",
       "      <th>Change_Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc</td>\n",
       "      <td>abc</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.003433</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abc</td>\n",
       "      <td>academic</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abc</td>\n",
       "      <td>academy</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.001014</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abc</td>\n",
       "      <td>acclaim</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.000654</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abc</td>\n",
       "      <td>acclaimed</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000785</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>zealand</td>\n",
       "      <td>york</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>zealand</td>\n",
       "      <td>young</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.004125</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>zealand</td>\n",
       "      <td>younger</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>zealand</td>\n",
       "      <td>youth</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>zealand</td>\n",
       "      <td>zealand</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>0.016040</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.001526</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>993129 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature1   Feature2  Wiki_Prob  Gen_Prob  Wiki_Bucket    Change  \\\n",
       "0           abc        abc   0.007721  0.004287            5 -0.003433   \n",
       "1           abc   academic   0.000065  0.000000            1 -0.000065   \n",
       "2           abc    academy   0.001014  0.000000            4 -0.001014   \n",
       "3           abc    acclaim   0.000654  0.000000            3 -0.000654   \n",
       "4           abc  acclaimed   0.000785  0.000000            4 -0.000785   \n",
       "...         ...        ...        ...       ...          ...       ...   \n",
       "999995  zealand       york   0.000784  0.001375            4  0.000591   \n",
       "999996  zealand      young   0.001673  0.004125            5  0.002452   \n",
       "999997  zealand    younger   0.000314  0.000917            2  0.000603   \n",
       "999998  zealand      youth   0.000261  0.000458            1  0.000197   \n",
       "999999  zealand    zealand   0.017567  0.016040            5 -0.001526   \n",
       "\n",
       "       Change_Direction  \n",
       "0              Decrease  \n",
       "1              Decrease  \n",
       "2              Decrease  \n",
       "3              Decrease  \n",
       "4              Decrease  \n",
       "...                 ...  \n",
       "999995         Increase  \n",
       "999996         Increase  \n",
       "999997         Increase  \n",
       "999998         Increase  \n",
       "999999         Decrease  \n",
       "\n",
       "[993129 rows x 7 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Wiki_Prob</th>\n",
       "      <th>Gen_Prob</th>\n",
       "      <th>Wiki_Bucket</th>\n",
       "      <th>Change</th>\n",
       "      <th>Change_Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.010112</td>\n",
       "      <td>0.008480</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.001632</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>old</td>\n",
       "      <td>year</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.008529</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>professional</td>\n",
       "      <td>year</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>0.008971</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>player</td>\n",
       "      <td>year</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>play</td>\n",
       "      <td>year</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.009227</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107579</th>\n",
       "      <td>involved</td>\n",
       "      <td>frequently</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.000150</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107580</th>\n",
       "      <td>work</td>\n",
       "      <td>frequently</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.000439</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107581</th>\n",
       "      <td>frequently</td>\n",
       "      <td>frequently</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107582</th>\n",
       "      <td>community</td>\n",
       "      <td>frequently</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107583</th>\n",
       "      <td>event</td>\n",
       "      <td>frequently</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107584 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature1    Feature2  Probability  Wiki_Prob  Gen_Prob  \\\n",
       "0               year        year     0.008422   0.010112  0.008480   \n",
       "1                old        year     0.005941   0.008529  0.009755   \n",
       "2       professional        year     0.007425   0.008112  0.008971   \n",
       "3             player        year     0.009722   0.007564  0.011081   \n",
       "4               play        year     0.007469   0.007449  0.009227   \n",
       "...              ...         ...          ...        ...       ...   \n",
       "107579      involved  frequently     0.000535   0.000439  0.000289   \n",
       "107580          work  frequently     0.000412   0.000665  0.000226   \n",
       "107581    frequently  frequently     0.000000   0.001661  0.002789   \n",
       "107582     community  frequently     0.000699   0.000592  0.000219   \n",
       "107583         event  frequently     0.000724   0.000427  0.000237   \n",
       "\n",
       "        Wiki_Bucket    Change Change_Direction  \n",
       "0               5.0 -0.001632         Decrease  \n",
       "1               5.0  0.001226         Increase  \n",
       "2               5.0  0.000859         Increase  \n",
       "3               5.0  0.003517         Increase  \n",
       "4               5.0  0.001778         Increase  \n",
       "...             ...       ...              ...  \n",
       "107579          2.0 -0.000150         Decrease  \n",
       "107580          3.0 -0.000439         Decrease  \n",
       "107581          5.0  0.001129         Increase  \n",
       "107582          3.0 -0.000373         Decrease  \n",
       "107583          2.0 -0.000190         Decrease  \n",
       "\n",
       "[107584 rows x 8 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_df = pd.DataFrame(hallucination_pairs_m, columns=['Feature1', 'Feature2', 'Probability'])\n",
    "\n",
    "merged_df = pd.merge(hallucination_df, df, on=['Feature1', 'Feature2'], how='left')\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_stats = merged_df.groupby('Wiki_Bucket')['Feature1'].count().reset_index()\n",
    "bucket_stats.columns = ['Wiki_Bucket', 'Count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.30431952992804"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the total count of all buckets\n",
    "total_count = bucket_stats['Count'].sum()\n",
    "\n",
    "# Get the count of bucket 5 (index 4)\n",
    "bucket_5_count = bucket_stats.loc[4, 'Count']\n",
    "\n",
    "# Calculate the percentage\n",
    "bucket_5_percentage = (bucket_5_count / total_count) * 100\n",
    "bucket_5_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wiki_Bucket</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>23483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>49804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wiki_Bucket  Count\n",
       "0          1.0   6013\n",
       "1          2.0  11708\n",
       "2          3.0  16550\n",
       "3          4.0  23483\n",
       "4          5.0  49804"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_stats_file = \"output/hallu/bucket_statistics.csv\"\n",
    "bucket_stats.to_csv(bucket_stats_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_report = merged_df[['Feature1', 'Feature2', 'Probability', 'Wiki_Bucket']]\n",
    "\n",
    "detailed_report_file = \"output/hallu/detailed_hallucination_report.csv\"\n",
    "detailed_report.to_csv(detailed_report_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factscore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
