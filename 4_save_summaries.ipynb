{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_infobox_line(line):\n",
    "    \"\"\"Extracts and concatenates occupations from the infobox line.\"\"\"\n",
    "    occupations = set()\n",
    "    tokens = line.split('\\t')\n",
    "    occupation_parts = []\n",
    "    for token in tokens:\n",
    "        if 'occupation_' in token:\n",
    "            # Extract the part of the occupation from the token\n",
    "            part = token.split(':', 1)[1].strip()\n",
    "            part = part.replace('[[', '').replace(']]', '').replace('*', '').strip()\n",
    "            occupation_parts.append(part)\n",
    "\n",
    "    # Join all parts into one string and add to the set of occupations\n",
    "    if occupation_parts:\n",
    "        full_occupation = \" \".join(occupation_parts)\n",
    "        occupations.add(full_occupation)\n",
    "\n",
    "    return occupations\n",
    "\n",
    "\n",
    "def remove_substrings(occupation_list):\n",
    "    result = []\n",
    "    for occupation in occupation_list:\n",
    "        if not any(occupation in other for other in occupation_list if occupation != other):\n",
    "            result.append(occupation)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_occupation_summaries(dataset_dir, occupations_set, char_threshold, num_samples=None):\n",
    "    \"\"\"Collects articles based on specified occupations up to a sample limit.\"\"\"\n",
    "    articles = defaultdict(list)\n",
    "    subsets = ['train/train', 'valid/valid', 'test/test']\n",
    "    title_files = [os.path.join(dataset_dir, f\"{subset}.title\") for subset in subsets]\n",
    "    box_files = [os.path.join(dataset_dir, f\"{subset}.box\") for subset in subsets]\n",
    "    sent_files = [os.path.join(dataset_dir, f\"{subset}.sent\") for subset in subsets]\n",
    "    nb_files = [os.path.join(dataset_dir, f\"{subset}.nb\") for subset in subsets]\n",
    "    \n",
    "    for idx, (title_file, box_file, sent_file, nb_file) in tqdm(enumerate(zip(title_files, box_files, sent_files, nb_files)), total=len(title_files), desc=\"Processing files\"):\n",
    "        with open(title_file, 'r') as tfile, open(box_file, 'r') as bfile, open(sent_file, 'r') as sfile, open(nb_file, 'r') as nfile:\n",
    "            title_lines = tfile.readlines()\n",
    "            sent_lines = sfile.readlines()\n",
    "            nb_lines = [int(line.strip()) for line in nfile.readlines()]\n",
    "            start_index = 0\n",
    "            for i, (bline, num_sentences) in enumerate(zip(bfile, nb_lines)):\n",
    "                if num_samples is not None and all(len(articles) >= num_samples):\n",
    "                    break\n",
    "                # print(\"bline:\", bline)\n",
    "                entry_occupations = parse_infobox_line(bline)\n",
    "                # print(\"entry:\", entry_occupations)\n",
    "                matched_occupations = [o for o in occupations_set if any(o in occ.lower() for occ in entry_occupations)]\n",
    "                # print(\"matched\", matched_occupations)\n",
    "                matched_occupations = remove_substrings(matched_occupations)\n",
    "                # print(\"matched2\", matched_occupations)\n",
    "                if matched_occupations:\n",
    "                    summary = ' '.join(sent_lines[start_index:start_index + num_sentences]).strip()\n",
    "                    name = title_lines[i].strip()\n",
    "                    if len(summary) > char_threshold:\n",
    "                        articles[name] = summary\n",
    "                start_index += num_sentences\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "occupations_set = {'organist', 'narrator', 'pastor', 'musician', 'narration', 'author', 'arranger', 'educator', 'military officer', 'soloist', 'model', 'naval surgeon', 'fitness instructor', 'broadcasting', 'composer', 'data scientist', 'economist', 'journalist', 'politician', 'host', 'film director', 'guitarist', 'environmentalist', 'songwriter', 'lawyer', 'radio broadcaster', 'screenwriter', 'athlete', 'coach', 'revolutionary', 'essayist', 'comedian', 'locksmith', 'writer', 'record producer', 'entertainer', 'dancer', 'stage', 'media executive', 'actress', 'parliamentarian', 'poet', 'businessman', 'model', 'actor', 'tv personality', 'songwriter', 'professor', 'mountaineer', 'radio host', 'travel writer', 'sportsperson', 'producer', 'film actress', 'philanthropist', 'businesswoman', 'voice actor', 'geographer', 'director', 'architect', 'teacher', 'television host', 'playwright', 'animal-rights activist', 'singer', 'translator', 'novelist', 'rapper', 'deejay', 'film producer', 'entrepreneur', 'stuntman', 'sportsman', 'columnist'}\n",
    "\n",
    "dataset_dir = 'wikipedia-biography-dataset/wikipedia-biography-dataset'\n",
    "char_threshold = 200\n",
    "num_samples = None\n",
    "\n",
    "articles_dict = get_occupation_summaries(dataset_dir, occupations_set, char_threshold, num_samples)\n",
    "\n",
    "with open('output/occupation_summaries_200.json', 'w') as json_file:\n",
    "    json.dump(articles_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and get frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'output/occupation_summaries_200.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    summaries = json.load(file)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "custom_stop_words = {'also', 'known', 'later', 'lxi', 'lx'} # should we use all nouns only? should we exclude names?\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "def process_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    return words\n",
    "\n",
    "word_counts = Counter()\n",
    "for entity, summary in summaries.items():\n",
    "    words = process_text(summary)\n",
    "    # print(words)\n",
    "    word_counts.update(words)\n",
    "\n",
    "sorted_word_counts = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240157"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_counts_first_500 = {k: sorted_word_counts[k] for k in list(sorted_word_counts)[:500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/freq_words/frequent_word_500.json', 'w') as file:\n",
    "    json.dump(sorted_word_counts_first_500, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "sorted_word_counts_first_n = {k: sorted_word_counts[k] for k in list(sorted_word_counts)[:n]}\n",
    "with open('output/freq_words/frequent_word_'+str(n)+'.json', 'w') as file:\n",
    "    json.dump(sorted_word_counts_first_n, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "sorted_word_counts_first_n = {k: sorted_word_counts[k] for k in list(sorted_word_counts)[:n]}\n",
    "with open('output/freq_words/frequent_word_'+str(n)+'.json', 'w') as file:\n",
    "    json.dump(sorted_word_counts_first_n, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factscore",
   "language": "python",
   "name": "factscore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
