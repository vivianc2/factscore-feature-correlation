{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_set(json_data):\n",
    "    feature_set = set()\n",
    "    for occupation, info in json_data.items():\n",
    "        for feature in info['attribute']:\n",
    "            # print(feature)\n",
    "            feature_set.add(feature)\n",
    "    return feature_set\n",
    "\n",
    "data = load_data('output/occupation_attribute_100.json')\n",
    "feature_set = get_feature_set(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def build_probability_matrix_from_list(data, feature_set):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    feature_counts = Counter()\n",
    "    co_occurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # Process each occupation group in the data\n",
    "    for summary in tqdm(data, desc=\"Processing occupations\"):\n",
    "        words = word_tokenize(summary.lower())\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word.isalpha()]\n",
    "        features_in_summary = [word for word in lemmatized_words if word in feature_set]\n",
    "\n",
    "        in_this_summary = set()\n",
    "\n",
    "        for i in range(len(features_in_summary)):\n",
    "            for j in range(i + 1, len(features_in_summary)):\n",
    "                if (features_in_summary[i],features_in_summary[j]) not in in_this_summary:\n",
    "                    in_this_summary.add((features_in_summary[i], features_in_summary[j]))\n",
    "                    feature_counts[features_in_summary[i]] += 1\n",
    "                    co_occurrence_counts[features_in_summary[i]][features_in_summary[j]] += 1\n",
    "\n",
    "    feature_list = sorted(feature_set)\n",
    "    matrix_size = len(feature_list)\n",
    "    probability_matrix = np.zeros((matrix_size, matrix_size))\n",
    "    \n",
    "    for i, f1 in enumerate(feature_list):\n",
    "        total_co_occurrences = sum(co_occurrence_counts[f1].values())\n",
    "        for j, f2 in enumerate(feature_list):\n",
    "            if f1 != f2 and total_co_occurrences > 0:\n",
    "                probability_matrix[i, j] = co_occurrence_counts[f1][f2] / total_co_occurrences\n",
    "\n",
    "    return feature_list, probability_matrix, feature_counts, co_occurrence_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing occupations: 100%|██████████| 105840/105840 [01:47<00:00, 983.23it/s] \n"
     ]
    }
   ],
   "source": [
    "# json_file = 'output/occupation_data_100.json'\n",
    "json_file = 'output/occupation_summaries_200.json'\n",
    "data = load_data(json_file)\n",
    "wiki_summary_list = [i for i in data.values()]\n",
    "feature_list, prob_matrix, feature_counts, co_occurrence_counts = build_probability_matrix_from_list(wiki_summary_list, feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_outputs(feature_list, probability_matrix, feature_counts, co_occurrence_counts, output_prefix):\n",
    "    # Save the feature list\n",
    "    with open(f'{output_prefix}_feature_list.json', 'w') as f:\n",
    "        json.dump(feature_list, f, indent=4)\n",
    "    \n",
    "    # Save the probability matrix using NumPy's save function\n",
    "    np.save(f'{output_prefix}_probability_matrix.npy', probability_matrix)\n",
    "    \n",
    "    # Save the feature counts\n",
    "    with open(f'{output_prefix}_feature_counts.json', 'w') as f:\n",
    "        json.dump(dict(feature_counts), f, indent=4)\n",
    "    \n",
    "    # Save the co_occurrence counts\n",
    "    with open(f'{output_prefix}_co_occurrence_counts.json', 'w') as f:\n",
    "        # Convert defaultdict to a normal dictionary for JSON serialization\n",
    "        co_occurrence_dict = {k: dict(v) for k, v in co_occurrence_counts.items()}\n",
    "        json.dump(co_occurrence_dict, f, indent=4)\n",
    "\n",
    "save_outputs(feature_list, prob_matrix, feature_counts, co_occurrence_counts, 'output/wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sorted_probabilities(feature_list, probability_matrix, feature_counts, co_occurrence_counts, output_file):\n",
    "    feature_index = {feature: idx for idx, feature in enumerate(feature_list)}\n",
    "    sorted_probabilities = []\n",
    "\n",
    "    # Collect all relevant data\n",
    "    for f1 in feature_list:\n",
    "        for f2 in feature_list:\n",
    "            if f1 != f2:\n",
    "                f1_idx = feature_index[f1]\n",
    "                f2_idx = feature_index[f2]\n",
    "                prob = probability_matrix[f1_idx][f2_idx]\n",
    "                f1_count = feature_counts[f1]\n",
    "                co_occurrence = co_occurrence_counts[f1][f2]\n",
    "                sorted_probabilities.append(((f1, f2), prob, f1_count, co_occurrence))\n",
    "\n",
    "    # Sort by probability, descending\n",
    "    sorted_probabilities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_file, 'w') as file:\n",
    "        for entry in sorted_probabilities:\n",
    "            line = f\"{entry[0]}: Probability={entry[1]:.4f}, Count of {entry[0][0]}={entry[2]}, Count of {entry[0][0]} followed by {entry[0][1]}={entry[3]}\\n\"\n",
    "            file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'wiki_sorted_probabilities_report.txt'\n",
    "save_sorted_probabilities(feature_list, prob_matrix, feature_counts, co_occurrence_counts, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_data_from_jsonl(jsonl_files):\n",
    "    data = []\n",
    "    for file_path in jsonl_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Each line is a complete JSON object\n",
    "                entry = json.loads(line)\n",
    "                data.append(entry)\n",
    "    return data\n",
    "\n",
    "labeled_path = Path('data/unlabeled')\n",
    "jsonl_files = list(labeled_path.glob('*.jsonl'))\n",
    "all_data = load_data_from_jsonl(jsonl_files)\n",
    "gen_output_list = [i['output'] for i in all_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing occupations: 100%|██████████| 6000/6000 [00:07<00:00, 818.90it/s] \n"
     ]
    }
   ],
   "source": [
    "feature_list_gen, prob_matrix_gen, feature_counts_gen, co_occurrence_counts_gen = build_probability_matrix_from_list(gen_output_list, feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_outputs(feature_list_gen, prob_matrix_gen, feature_counts_gen, co_occurrence_counts_gen, 'output/gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'gen_sorted_probabilities_report.txt'\n",
    "save_sorted_probabilities(feature_list_gen, prob_matrix_gen, feature_counts_gen, co_occurrence_counts_gen, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_probability_matrices(wiki_matrix, gen_matrix, feature_list, output_file):\n",
    "\n",
    "    matrix_size = len(feature_list)\n",
    "    differences = np.zeros((matrix_size, matrix_size))\n",
    "    \n",
    "    for i in range(matrix_size):\n",
    "        for j in range(matrix_size):\n",
    "            if i != j:  # We skip comparing a feature with itself\n",
    "                differences[i][j] = wiki_matrix[i][j] - gen_matrix[i][j]\n",
    "\n",
    "    # print(differences)\n",
    "\n",
    "    flat_differences = []\n",
    "    for i in range(matrix_size):\n",
    "        for j in range(matrix_size):\n",
    "            if i != j:\n",
    "                flat_differences.append(((feature_list[i], feature_list[j]), differences[i][j]))\n",
    "\n",
    "    flat_differences.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    significant_differences = {\n",
    "        'most_positive': flat_differences[:20], \n",
    "        'most_negative': flat_differences[-20:] \n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(significant_differences, f, indent=4)\n",
    "\n",
    "    return significant_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n",
      "443\n"
     ]
    }
   ],
   "source": [
    "print(len(prob_matrix))\n",
    "print(len(feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wiki_prob_matrix = prob_matrix\n",
    "gen_prob_matrix = prob_matrix_gen\n",
    "feature_list = feature_list\n",
    "\n",
    "output_file_path = 'comparison_results.json'\n",
    "comparison_results = compare_probability_matrices(wiki_prob_matrix, gen_prob_matrix, feature_list, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build buckets and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def flatten_matrix(matrix, feature_list):\n",
    "    flattened_data = []\n",
    "    for i in range(len(feature_list)):\n",
    "        for j in range(len(feature_list)):\n",
    "            if i != j:  # Exclude diagonal elements\n",
    "                value = matrix[i, j]\n",
    "                if value > 0:\n",
    "                    flattened_data.append((feature_list[i], feature_list[j], value))\n",
    "    return flattened_data\n",
    "\n",
    "wiki_flattened = flatten_matrix(prob_matrix, feature_list)\n",
    "gen_flattened = flatten_matrix(prob_matrix_gen, feature_list_gen)\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_flattened, columns=['Feature1', 'Feature2', 'Wiki_Prob'])\n",
    "gen_df = pd.DataFrame(gen_flattened, columns=['Feature1', 'Feature2', 'Gen_Prob'])\n",
    "\n",
    "comparison_df = pd.merge(wiki_df, gen_df, on=['Feature1', 'Feature2'], how='inner')\n",
    "\n",
    "comparison_df['Wiki_Bucket'] = pd.qcut(comparison_df['Wiki_Prob'], 5, labels=False) + 1\n",
    "comparison_df['Gen_Bucket'] = pd.qcut(comparison_df['Gen_Prob'], 5, labels=False) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Wiki_Prob</th>\n",
       "      <th>Gen_Prob</th>\n",
       "      <th>Wiki_Bucket</th>\n",
       "      <th>Gen_Bucket</th>\n",
       "      <th>Change</th>\n",
       "      <th>Change_Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acting</td>\n",
       "      <td>actor</td>\n",
       "      <td>0.015059</td>\n",
       "      <td>0.021979</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acting</td>\n",
       "      <td>actress</td>\n",
       "      <td>0.009742</td>\n",
       "      <td>0.015448</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acting</td>\n",
       "      <td>address</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acting</td>\n",
       "      <td>air</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000903</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acting</td>\n",
       "      <td>album</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.006029</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85303</th>\n",
       "      <td>york</td>\n",
       "      <td>world</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.012391</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>Increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85304</th>\n",
       "      <td>york</td>\n",
       "      <td>writer</td>\n",
       "      <td>0.006027</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.002625</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>york</td>\n",
       "      <td>writing</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85306</th>\n",
       "      <td>york</td>\n",
       "      <td>wrote</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.002271</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85307</th>\n",
       "      <td>york</td>\n",
       "      <td>year</td>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.004655</td>\n",
       "      <td>Decrease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85308 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature1 Feature2  Wiki_Prob  Gen_Prob  Wiki_Bucket  Gen_Bucket  \\\n",
       "0       acting    actor   0.015059  0.021979            5           5   \n",
       "1       acting  actress   0.009742  0.015448            5           5   \n",
       "2       acting  address   0.000116  0.000126            1           1   \n",
       "3       acting      air   0.001029  0.000126            2           1   \n",
       "4       acting    album   0.003610  0.006029            4           4   \n",
       "...        ...      ...        ...       ...          ...         ...   \n",
       "85303     york    world   0.008073  0.012391            5           5   \n",
       "85304     york   writer   0.006027  0.003401            5           4   \n",
       "85305     york  writing   0.005166  0.004252            4           4   \n",
       "85306     york    wrote   0.005308  0.003037            4           4   \n",
       "85307     york     year   0.014373  0.009718            5           5   \n",
       "\n",
       "         Change Change_Direction  \n",
       "0      0.006920         Increase  \n",
       "1      0.005706         Increase  \n",
       "2      0.000009         Increase  \n",
       "3     -0.000903         Decrease  \n",
       "4      0.002419         Increase  \n",
       "...         ...              ...  \n",
       "85303  0.004318         Increase  \n",
       "85304 -0.002625         Decrease  \n",
       "85305 -0.000914         Decrease  \n",
       "85306 -0.002271         Decrease  \n",
       "85307 -0.004655         Decrease  \n",
       "\n",
       "[85308 rows x 8 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen_Bucket      1     2     3     4      5\n",
      "Wiki_Bucket                               \n",
      "1            9383  4258  2205   892    325\n",
      "2            4885  5516  3963  2007    689\n",
      "3            2119  4664  5158  3661   1461\n",
      "4             618  2261  4517  6095   3569\n",
      "5              73   366  1218  4387  11018\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate the stats of how many pairs move from each original bucket to each new bucket\n",
    "bucket_movement = comparison_df.groupby(['Wiki_Bucket', 'Gen_Bucket']).size().unstack(fill_value=0)\n",
    "print(bucket_movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 5 Change Stats:\n",
      "Change_Direction\n",
      "Decrease    8788\n",
      "Increase    8274\n",
      "dtype: int64\n",
      "\n",
      "Bucket 5 Change Values:\n",
      "count    17062.000000\n",
      "mean         0.002226\n",
      "std          0.011289\n",
      "min         -0.151061\n",
      "25%         -0.003586\n",
      "50%         -0.000254\n",
      "75%          0.005340\n",
      "max          0.490012\n",
      "Name: Change, dtype: float64\n",
      "\n",
      "Bucket 1 Change Stats:\n",
      "Change_Direction\n",
      "Decrease     5019\n",
      "Increase    12044\n",
      "dtype: int64\n",
      "\n",
      "Bucket 1 Change Values:\n",
      "count    17063.000000\n",
      "mean         0.000779\n",
      "std          0.002287\n",
      "min         -0.000676\n",
      "25%         -0.000041\n",
      "50%          0.000212\n",
      "75%          0.000825\n",
      "max          0.110528\n",
      "Name: Change, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Add columns to check if each pair's probability increased, decreased, or stayed the same\n",
    "comparison_df['Change'] = comparison_df['Gen_Prob'] - comparison_df['Wiki_Prob']\n",
    "comparison_df['Change_Direction'] = comparison_df['Change'].apply(lambda x: 'Increase' if x > 0 else ('Decrease' if x < 0 else 'No Change'))\n",
    "\n",
    "# 2. Analyze the change for original bucket 5\n",
    "bucket_5_change_stats = comparison_df[comparison_df['Wiki_Bucket'] == 5].groupby('Change_Direction').size()\n",
    "\n",
    "# 3. Analyze the change for original bucket 1\n",
    "bucket_1_change_stats = comparison_df[comparison_df['Wiki_Bucket'] == 1].groupby('Change_Direction').size()\n",
    "\n",
    "# Summary of average and median changes for bucket 5 and bucket 1\n",
    "bucket_5_change_values = comparison_df[comparison_df['Wiki_Bucket'] == 5]['Change'].describe()\n",
    "bucket_1_change_values = comparison_df[comparison_df['Wiki_Bucket'] == 1]['Change'].describe()\n",
    "\n",
    "# Print the results\n",
    "print(\"Bucket 5 Change Stats:\")\n",
    "print(bucket_5_change_stats)\n",
    "print(\"\\nBucket 5 Change Values:\")\n",
    "print(bucket_5_change_values)\n",
    "\n",
    "print(\"\\nBucket 1 Change Stats:\")\n",
    "print(bucket_1_change_stats)\n",
    "print(\"\\nBucket 1 Change Values:\")\n",
    "print(bucket_1_change_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factscore",
   "language": "python",
   "name": "factscore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
