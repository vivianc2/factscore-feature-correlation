{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_set(json_data):\n",
    "    feature_set = set()\n",
    "    for occupation, info in json_data.items():\n",
    "        for feature in info['attribute']:\n",
    "            # print(feature)\n",
    "            feature_set.add(feature)\n",
    "    return feature_set\n",
    "\n",
    "data = load_data('output/occupation_attribute_100.json')\n",
    "feature_set = get_feature_set(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work because it's only looking at sentence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from collections import defaultdict, Counter\n",
    "# import nltk\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Ensure required NLTK resources are downloaded\n",
    "# nltk.download('punkt')  # Tokenizer\n",
    "# nltk.download('wordnet')  # Lemmatization data\n",
    "\n",
    "# def load_and_process_sentences(files):\n",
    "#     sentences = []\n",
    "#     for file_path in files:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             sentences.extend(file.readlines())\n",
    "#     return sentences\n",
    "\n",
    "# def build_probability_matrix(sentences, feature_set):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     feature_counts = Counter()\n",
    "#     co_occurrence_counts = defaultdict(Counter)\n",
    "    \n",
    "#     for sentence in tqdm(sentences, desc=\"Processing Sentences\"):\n",
    "#         words = word_tokenize(sentence.lower())\n",
    "#         lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word.isalpha()]\n",
    "#         features_in_sentence = [word for word in lemmatized_words if word in feature_set]\n",
    "        \n",
    "#         # Track seen features in this sentence to prevent double counting in the context\n",
    "#         seen_features = set()\n",
    "        \n",
    "#         for i, feature in enumerate(features_in_sentence):\n",
    "#             if feature not in seen_features:\n",
    "#                 feature_counts[feature] += 1\n",
    "#                 seen_features.add(feature)\n",
    "            \n",
    "#             # Count only subsequent features different from the current feature\n",
    "#             for subsequent_feature in features_in_sentence[i+1:]:\n",
    "#                 if subsequent_feature != feature:\n",
    "#                     co_occurrence_counts[feature][subsequent_feature] += 1\n",
    "\n",
    "#     feature_list = sorted(feature_set)\n",
    "#     matrix_size = len(feature_list)\n",
    "#     probability_matrix = np.zeros((matrix_size, matrix_size))\n",
    "#     feature_index = {feature: idx for idx, feature in enumerate(feature_list)}\n",
    "    \n",
    "#     for f1 in feature_list:\n",
    "#         f1_idx = feature_index[f1]\n",
    "#         for f2 in feature_list:\n",
    "#             f2_idx = feature_index[f2]\n",
    "#             if feature_counts[f1] > 0:\n",
    "#                 probability_matrix[f1_idx][f2_idx] = co_occurrence_counts[f1][f2] / feature_counts[f1]\n",
    "\n",
    "#     return feature_list, probability_matrix, feature_counts, co_occurrence_counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dir = 'wikipedia-biography-dataset/wikipedia-biography-dataset'\n",
    "# subsets = ['train/train']#, 'valid/valid', 'test/test']\n",
    "# sent_files = [os.path.join(dataset_dir, f\"{subset}.sent\") for subset in subsets]\n",
    "# sentences = load_and_process_sentences(sent_files)\n",
    "\n",
    "# feature_list, probability_matrix, feature_counts, co_occurrence_counts = build_probability_matrix(sentences, feature_set)\n",
    "\n",
    "# # print(\"Feature List:\", feature_list)\n",
    "# # print(\"Probability Matrix:\\n\", prob_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def build_probability_matrix_from_list(data, feature_set):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    feature_counts = Counter()\n",
    "    co_occurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # Process each occupation group in the data\n",
    "    for summary in tqdm(data, desc=\"Processing occupations\"):\n",
    "        words = word_tokenize(summary.lower())\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word.isalpha()]\n",
    "        features_in_summary = [word for word in lemmatized_words if word in feature_set]\n",
    "\n",
    "        in_this_summary = set()\n",
    "\n",
    "        for i in range(len(features_in_summary)):\n",
    "            for j in range(i + 1, len(features_in_summary)):\n",
    "                if (features_in_summary[i],features_in_summary[j]) not in in_this_summary:\n",
    "                    in_this_summary.add((features_in_summary[i], features_in_summary[j]))\n",
    "                    feature_counts[features_in_summary[i]] += 1\n",
    "                    co_occurrence_counts[features_in_summary[i]][features_in_summary[j]] += 1\n",
    "\n",
    "    feature_list = sorted(feature_set)\n",
    "    matrix_size = len(feature_list)\n",
    "    probability_matrix = np.zeros((matrix_size, matrix_size))\n",
    "    \n",
    "    for i, f1 in enumerate(feature_list):\n",
    "        total_co_occurrences = sum(co_occurrence_counts[f1].values())\n",
    "        for j, f2 in enumerate(feature_list):\n",
    "            if f1 != f2 and total_co_occurrences > 0:\n",
    "                probability_matrix[i, j] = co_occurrence_counts[f1][f2] / total_co_occurrences\n",
    "\n",
    "    return feature_list, probability_matrix, feature_counts, co_occurrence_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing occupations: 100%|██████████| 105840/105840 [01:47<00:00, 983.23it/s] \n"
     ]
    }
   ],
   "source": [
    "# json_file = 'output/occupation_data_100.json'\n",
    "json_file = 'output/occupation_summaries_200.json'\n",
    "data = load_data(json_file)\n",
    "wiki_summary_list = [i for i in data.values()]\n",
    "feature_list, prob_matrix, feature_counts, co_occurrence_counts = build_probability_matrix_from_list(wiki_summary_list, feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_outputs(feature_list, probability_matrix, feature_counts, co_occurrence_counts, output_prefix):\n",
    "    # Save the feature list\n",
    "    with open(f'{output_prefix}_feature_list.json', 'w') as f:\n",
    "        json.dump(feature_list, f, indent=4)\n",
    "    \n",
    "    # Save the probability matrix using NumPy's save function\n",
    "    np.save(f'{output_prefix}_probability_matrix.npy', probability_matrix)\n",
    "    \n",
    "    # Save the feature counts\n",
    "    with open(f'{output_prefix}_feature_counts.json', 'w') as f:\n",
    "        json.dump(dict(feature_counts), f, indent=4)\n",
    "    \n",
    "    # Save the co_occurrence counts\n",
    "    with open(f'{output_prefix}_co_occurrence_counts.json', 'w') as f:\n",
    "        # Convert defaultdict to a normal dictionary for JSON serialization\n",
    "        co_occurrence_dict = {k: dict(v) for k, v in co_occurrence_counts.items()}\n",
    "        json.dump(co_occurrence_dict, f, indent=4)\n",
    "\n",
    "save_outputs(feature_list, prob_matrix, feature_counts, co_occurrence_counts, 'output/wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sorted_probabilities(feature_list, probability_matrix, feature_counts, co_occurrence_counts, output_file):\n",
    "    feature_index = {feature: idx for idx, feature in enumerate(feature_list)}\n",
    "    sorted_probabilities = []\n",
    "\n",
    "    # Collect all relevant data\n",
    "    for f1 in feature_list:\n",
    "        for f2 in feature_list:\n",
    "            if f1 != f2:\n",
    "                f1_idx = feature_index[f1]\n",
    "                f2_idx = feature_index[f2]\n",
    "                prob = probability_matrix[f1_idx][f2_idx]\n",
    "                f1_count = feature_counts[f1]\n",
    "                co_occurrence = co_occurrence_counts[f1][f2]\n",
    "                sorted_probabilities.append(((f1, f2), prob, f1_count, co_occurrence))\n",
    "\n",
    "    # Sort by probability, descending\n",
    "    sorted_probabilities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_file, 'w') as file:\n",
    "        for entry in sorted_probabilities:\n",
    "            line = f\"{entry[0]}: Probability={entry[1]:.4f}, Count of {entry[0][0]}={entry[2]}, Count of {entry[0][0]} followed by {entry[0][1]}={entry[3]}\\n\"\n",
    "            file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'wiki_sorted_probabilities_report.txt'\n",
    "save_sorted_probabilities(feature_list, prob_matrix, feature_counts, co_occurrence_counts, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_data_from_jsonl(jsonl_files):\n",
    "    data = []\n",
    "    for file_path in jsonl_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Each line is a complete JSON object\n",
    "                entry = json.loads(line)\n",
    "                data.append(entry)\n",
    "    return data\n",
    "\n",
    "labeled_path = Path('data/unlabeled')\n",
    "jsonl_files = list(labeled_path.glob('*.jsonl'))\n",
    "all_data = load_data_from_jsonl(jsonl_files)\n",
    "gen_output_list = [i['output'] for i in all_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing occupations: 100%|██████████| 6000/6000 [00:07<00:00, 818.90it/s] \n"
     ]
    }
   ],
   "source": [
    "feature_list_gen, prob_matrix_gen, feature_counts_gen, co_occurrence_counts_gen = build_probability_matrix_from_list(gen_output_list, feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_outputs(feature_list_gen, prob_matrix_gen, feature_counts_gen, co_occurrence_counts_gen, 'output/gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'gen_sorted_probabilities_report.txt'\n",
    "save_sorted_probabilities(feature_list_gen, prob_matrix_gen, feature_counts_gen, co_occurrence_counts_gen, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_probability_matrices(wiki_matrix, gen_matrix, feature_list, output_file):\n",
    "\n",
    "    matrix_size = len(feature_list)\n",
    "    differences = np.zeros((matrix_size, matrix_size))\n",
    "    \n",
    "    for i in range(matrix_size):\n",
    "        for j in range(matrix_size):\n",
    "            if i != j:  # We skip comparing a feature with itself\n",
    "                differences[i][j] = wiki_matrix[i][j] - gen_matrix[i][j]\n",
    "\n",
    "    # print(differences)\n",
    "\n",
    "    flat_differences = []\n",
    "    for i in range(matrix_size):\n",
    "        for j in range(matrix_size):\n",
    "            if i != j:\n",
    "                flat_differences.append(((feature_list[i], feature_list[j]), differences[i][j]))\n",
    "\n",
    "    flat_differences.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    significant_differences = {\n",
    "        'most_positive': flat_differences[:20], \n",
    "        'most_negative': flat_differences[-20:] \n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(significant_differences, f, indent=4)\n",
    "\n",
    "    return significant_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n",
      "443\n"
     ]
    }
   ],
   "source": [
    "print(len(prob_matrix))\n",
    "print(len(feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wiki_prob_matrix = prob_matrix\n",
    "gen_prob_matrix = prob_matrix_gen\n",
    "feature_list = feature_list\n",
    "\n",
    "output_file_path = 'comparison_results.json'\n",
    "comparison_results = compare_probability_matrices(wiki_prob_matrix, gen_prob_matrix, feature_list, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factscore",
   "language": "python",
   "name": "factscore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
