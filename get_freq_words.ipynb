{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import wptools\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_occupations(text):\n",
    "    \"\"\"Extracts multiple occupations from complex formatted strings.\"\"\"\n",
    "    # Handle {{hlist|...}} and {{flatlist|...}}\n",
    "    if '{{' in text:\n",
    "        text = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', text)  # Simplify links\n",
    "        text = re.sub(r'{{[^|]*\\|', '', text)  # Remove the template up to the first '|'\n",
    "        text = re.sub(r'}}', '', text)  # Remove closing braces\n",
    "    # Handle <br> tags and strip unwanted characters\n",
    "    text = re.sub(r'<br\\s*/?>', '|', text)  # Replace <br> with |\n",
    "    \n",
    "    # Remove additional unwanted characters\n",
    "    text = re.sub(r'\\[\\[|\\]\\]', '', text)  # Remove double brackets\n",
    "    text = re.sub(r'\\* ', '', text)  # Remove asterisks with spaces\n",
    "\n",
    "    # Split by '|' and new lines as well as commas now\n",
    "    occupations = re.split(r'\\||\\n|,', text)\n",
    "    return [occupation.strip(\"*\").strip().lower() for occupation in occupations if occupation.strip()]\n",
    "\n",
    "\n",
    "def get_occupation_from_wikipedia(entity):\n",
    "    \"\"\"Use wptools to fetch and parse the occupation from Wikipedia without verbose output.\"\"\"\n",
    "    try:\n",
    "        page = wptools.page(entity, silent=True).get_parse(show=False)\n",
    "        infobox = page.data['infobox']\n",
    "        if 'occupation' in infobox:\n",
    "            return parse_occupations(infobox['occupation'])\n",
    "        else:\n",
    "            return [\"Occupation not listed\"]\n",
    "    except Exception as e:\n",
    "        return [f\"Error retrieving page: {str(e)}\"]\n",
    "\n",
    "def extract_occupations(filepath):\n",
    "    \"\"\"Extract occupations for entities in a JSONL file and count occurrences.\"\"\"\n",
    "    occupation_count = defaultdict(int)\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            entity = data.get('topic', 'Unknown')\n",
    "            occupations = get_occupation_from_wikipedia(entity)\n",
    "            for occupation in occupations:\n",
    "                occupation_count[occupation] += 1\n",
    "    return occupation_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'data/labeled/ChatGPT.jsonl'\n",
    "occupation_counts = extract_occupations(filepath)\n",
    "with open('occupation_counts.txt', 'w', encoding='utf-8') as f:\n",
    "    for occupation, count in sorted(occupation_counts.items(), key=lambda item: item[1], reverse=True):\n",
    "        f.write(f\"{occupation}, Count: {count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(occupation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'organist', 'narrator', 'pastor', 'musician', 'narration', 'author', 'arranger', 'educator', 'military officer', 'composer; soloist', 'model (person)', 'naval surgeon', 'film and television actor', 'singer fitness instructor', 'broadcasting', 'composer', 'data scientist', 'economist', 'voice acting in japan', 'journalist', 'politician', 'host', 'film director', 'guitarist', 'environmentalist', 'singer-songwriter', 'lawyer', 'radio broadcaster', 'screenwriter', 'athlete /coach (sports authority of india )', 'revolutionary', 'essayist', 'comedian', 'locksmith', 'writer', 'record producer', 'entertainer', 'dancer', 'stage', 'media executive', 'actress', 'parliamentarian', 'poet', 'businessman', 'model', 'actor', 'tv personality', 'songwriter', 'deejay (jamaican)', 'professor-emeritus of ecumenics and mission', 'mountaineer', 'radio host', 'travel writer', 'sportsperson', 'producer', 'film actress', 'philanthropist', 'businesswoman', 'voice actor', 'geographer', 'director', 'architect', 'teacher', 'politician and lawyer', 'television host', 'playwright', 'animal-rights activist', 'singer', 'translator', 'novelist', 'rapper', 'deejay', 'film producer', 'entrepreneur', 'television talk show host', 'stuntman', 'sportsman', 'author and columnist'}\n"
     ]
    }
   ],
   "source": [
    "occupations_set = set(occupation_counts.keys())\n",
    "occupations_set.remove(\"Occupation not listed\")\n",
    "occupations_set.remove(\"Error retrieving page: argument of type 'NoneType' is not iterable\")\n",
    "print(occupations_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations_set = {'organist', 'narrator', 'pastor', 'musician', 'narration', 'author', 'arranger', 'educator', 'military officer', 'soloist', 'model', 'naval surgeon', 'fitness instructor', 'broadcasting', 'composer', 'data scientist', 'economist', 'journalist', 'politician', 'host', 'film director', 'guitarist', 'environmentalist', 'songwriter', 'lawyer', 'radio broadcaster', 'screenwriter', 'athlete', 'coach', 'revolutionary', 'essayist', 'comedian', 'locksmith', 'writer', 'record producer', 'entertainer', 'dancer', 'stage', 'media executive', 'actress', 'parliamentarian', 'poet', 'businessman', 'model', 'actor', 'tv personality', 'songwriter', 'professor', 'mountaineer', 'radio host', 'travel writer', 'sportsperson', 'producer', 'film actress', 'philanthropist', 'businesswoman', 'voice actor', 'geographer', 'director', 'architect', 'teacher', 'television host', 'playwright', 'animal-rights activist', 'singer', 'translator', 'novelist', 'rapper', 'deejay', 'film producer', 'entrepreneur', 'stuntman', 'sportsman', 'columnist'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(occupations_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yc833/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_infobox_line(line):\n",
    "    \"\"\"Extracts and concatenates occupations from the infobox line.\"\"\"\n",
    "    occupations = set()\n",
    "    tokens = line.split('\\t')\n",
    "    occupation_parts = []\n",
    "    for token in tokens:\n",
    "        if 'occupation_' in token:\n",
    "            # Extract the part of the occupation from the token\n",
    "            part = token.split(':', 1)[1].strip()\n",
    "            part = part.replace('[[', '').replace(']]', '').replace('*', '').strip()\n",
    "            occupation_parts.append(part)\n",
    "\n",
    "    # Join all parts into one string and add to the set of occupations\n",
    "    if occupation_parts:\n",
    "        full_occupation = \" \".join(occupation_parts)\n",
    "        occupations.add(full_occupation)\n",
    "\n",
    "    return occupations\n",
    "\n",
    "\n",
    "def remove_substrings(occupation_list):\n",
    "    result = []\n",
    "    for occupation in occupation_list:\n",
    "        if not any(occupation in other for other in occupation_list if occupation != other):\n",
    "            result.append(occupation)\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_articles_by_occupation(dataset_dir, occupations_set, char_threshold, num_samples):\n",
    "    \"\"\"Collects articles based on specified occupations up to a sample limit.\"\"\"\n",
    "    articles = defaultdict(list)\n",
    "    subsets = ['train/train', 'valid/valid', 'test/test']\n",
    "    title_files = [os.path.join(dataset_dir, f\"{subset}.title\") for subset in subsets]\n",
    "    box_files = [os.path.join(dataset_dir, f\"{subset}.box\") for subset in subsets]\n",
    "    sent_files = [os.path.join(dataset_dir, f\"{subset}.sent\") for subset in subsets]\n",
    "    nb_files = [os.path.join(dataset_dir, f\"{subset}.nb\") for subset in subsets]\n",
    "\n",
    "    for idx, (title_file, box_file, sent_file, nb_file) in enumerate(zip(title_files, box_files, sent_files, nb_files)):\n",
    "        with open(title_file, 'r') as tfile, open(box_file, 'r') as bfile, open(sent_file, 'r') as sfile, open(nb_file, 'r') as nfile:\n",
    "            title_lines = tfile.readlines()\n",
    "            sent_lines = sfile.readlines()\n",
    "            nb_lines = [int(line.strip()) for line in nfile.readlines()]\n",
    "            start_index = 0\n",
    "            for i, (bline, num_sentences) in enumerate(zip(bfile, nb_lines)):\n",
    "                if all(len(articles[occ]) >= num_samples for occ in occupations_set):\n",
    "                    break\n",
    "                # print(\"bline:\", bline)\n",
    "                entry_occupations = parse_infobox_line(bline)\n",
    "                # print(\"entry:\", entry_occupations)\n",
    "                matched_occupations = [o for o in occupations_set if any(o in occ.lower() for occ in entry_occupations)]\n",
    "                # print(\"matched\", matched_occupations)\n",
    "                matched_occupations = remove_substrings(matched_occupations)\n",
    "                # print(\"matched2\", matched_occupations)\n",
    "                if matched_occupations:\n",
    "                    summary = ' '.join(sent_lines[start_index:start_index + num_sentences]).strip()\n",
    "                    name = title_lines[i].strip()\n",
    "                    if len(summary) > char_threshold:\n",
    "                        for occ in matched_occupations:\n",
    "                            if len(articles[occ]) < num_samples:\n",
    "                                articles[occ].append({'name': name, 'summary': summary})\n",
    "                                if len(articles[occ]) == num_samples:\n",
    "                                    break\n",
    "                start_index += num_sentences\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "def save_data_and_report(articles, data_output_path, report_output_path, attribute_output_path):\n",
    "    \"\"\"Saves detailed data to JSON and generates a report on frequent words, also saved to JSON.\"\"\"\n",
    "    standard_stop_words = set(stopwords.words('english'))\n",
    "    custom_stop_words = [\"born\", \"known\", \"also\", \"became\", \"name\", \"one\"]\n",
    "    stop_words = standard_stop_words.union(set(custom_stop_words))\n",
    "    data = {}\n",
    "    report_data = {}\n",
    "    attribute_data = {}\n",
    "\n",
    "    for occupation, entries in articles.items():\n",
    "        word_counter = Counter()\n",
    "        for entry in entries:\n",
    "            words = [word.lower() for word in word_tokenize(entry['summary']) if word.lower() not in stop_words and word.isalpha()]\n",
    "            word_counter.update(words)\n",
    "        \n",
    "        most_common_words = word_counter.most_common(20)\n",
    "        data[occupation] = {\n",
    "            'count': len(entries),\n",
    "            'entries': entries\n",
    "        }\n",
    "        report_data[occupation] = {\n",
    "            'count': len(entries),\n",
    "            'frequent_words': [{word: count} for word, count in most_common_words]\n",
    "        }\n",
    "        attribute_data[occupation] = {\n",
    "            'attribute': [word for word, _ in most_common_words]\n",
    "        }\n",
    "    \n",
    "    print(len(report_data))\n",
    "    print(report_data.keys())\n",
    "\n",
    "    with open(data_output_path, 'w', encoding='utf-8') as f_data:\n",
    "        json.dump(data, f_data, indent=4)\n",
    "\n",
    "    with open(report_output_path, 'w', encoding='utf-8') as f_report:\n",
    "        json.dump(report_data, f_report, indent=4)\n",
    "\n",
    "    with open(attribute_output_path, 'w', encoding='utf-8') as f_report:\n",
    "        json.dump(attribute_data, f_report, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "dict_keys(['singer', 'actress', 'businesswoman', 'politician', 'composer', 'arranger', 'musician', 'film director', 'actor', 'playwright', 'poet', 'writer', 'essayist', 'songwriter', 'travel writer', 'director', 'teacher', 'lawyer', 'screenwriter', 'record producer', 'film producer', 'architect', 'model', 'comedian', 'author', 'novelist', 'producer', 'professor', 'journalist', 'translator', 'coach', 'host', 'organist', 'rapper', 'radio host', 'entrepreneur', 'athlete', 'philanthropist', 'businessman', 'economist', 'revolutionary', 'environmentalist', 'stage', 'tv personality', 'dancer', 'educator', 'narration', 'guitarist', 'entertainer', 'voice actor', 'columnist', 'narrator', 'television host', 'media executive', 'sportsman', 'broadcasting', 'film actress', 'military officer', 'pastor', 'stuntman', 'radio broadcaster', 'deejay', 'geographer', 'mountaineer', 'soloist', 'sportsperson', 'fitness instructor', 'data scientist', 'parliamentarian', 'locksmith', 'naval surgeon'])\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = 'wikipedia-biography-dataset/wikipedia-biography-dataset'\n",
    "char_threshold = 200\n",
    "num_samples = 30\n",
    "data_output_path = 'output/occupation_data.json'\n",
    "report_output_path = 'output/occupation_report.json'\n",
    "attribute_output_path = 'output/occupation_attribute.json'\n",
    "\n",
    "articles_by_occupation = filter_articles_by_occupation(dataset_dir, occupations_set, char_threshold, num_samples)\n",
    "save_data_and_report(articles_by_occupation, data_output_path, report_output_path, attribute_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now getting the correct summaries. Still some problem with occupation. Seeing slashes \"actor/screenwriter/executive\", but no spaces\n",
    "2. Now using occupation from occupation_set, but not parsing from info box correctly. \n",
    "3. Changed from set to string, and can now have occupations with space.\n",
    "4. changed from 20 to 30\n",
    "problem: writer is also included for song writer\n",
    "    occupation_1:film\n",
    "    occupation_2:director\n",
    "    occupation_3:,\n",
    "    occupation_4:film\n",
    "    occupation_5:producer\n",
    "5. fixed the above. Now if you have 'writer' and 'song writer' in matched, we only keep 'song writer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factscore",
   "language": "python",
   "name": "factscore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
